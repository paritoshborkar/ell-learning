{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from fastai.imports import *\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "path_to_raw_data = Path(\"../data/raw\")\n",
    "df = pd.read_csv(path_to_raw_data/\"train.csv\")\n",
    "eval_df = pd.read_csv(path_to_raw_data / \"test.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "        text_id   \n0  0016926B079C  \\\n1  0022683E9EA5   \n2  00299B378633   \n3  003885A45F42   \n4  0049B1DF5CCC   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 full_text   \n0  I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\\n\\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and st...  \\\n1  When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A\\n\\nproblem is a change for you because it can make you see different and help you to understand how tings wok.\\n\\nFirst of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me bec...   \n2  Dear, Principal\\n\\nIf u change the school policy of having a grade b average that unfair. Because many students have a C average. So that means that they cant go out for sports or other activities they want to do bad. That's like taking everything they have. What if kids want to become good at something, but now they cant because of that school policy. If they have a C average they should still be able to go out for sports or activities. A C average isn't that bad, its higher then a D average. If the school police was if you have a D average of lower they shouldn't do sports or activities....   \n3  The best time in life is when you become yourself. I agree that the greatest accomplishment, is when you be yourself in a world that constantly trying to make you something else. Because you make your own choices, you become more happy, and you respect others.\\n\\nFirst, you make your own choices by being yourself. Becoming yourself means that you should be able to make your own choices and not be shy or afraid of what you're doing. Because you're defining yourself by doing those things that you want. Some people follow others, therefore, they don't make their own choices. People are afraid...   \n4  Small act of kindness can impact in other people can change people to become better persons you can have an impact of kindess with a homeles that can change his life or with some who needed they are going to know you are a nice person if you are a nice person everywhere you go people is going to like your personality so you have to be a nice person with others like a old women triying to cross the road thats a impact of kindness when you do that you feel a greate person you can change people in the way they think by helping others treating nice other people give them some advice when you s...   \n\n   cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n0       3.5     3.5         3.0          3.0      4.0          3.0  \n1       2.5     2.5         3.0          2.0      2.0          2.5  \n2       3.0     3.5         3.0          3.0      3.0          2.5  \n3       4.5     4.5         4.5          4.5      4.0          5.0  \n4       2.5     3.0         3.0          3.0      2.5          2.5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_id</th>\n      <th>full_text</th>\n      <th>cohesion</th>\n      <th>syntax</th>\n      <th>vocabulary</th>\n      <th>phraseology</th>\n      <th>grammar</th>\n      <th>conventions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0016926B079C</td>\n      <td>I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\\n\\nThe hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and st...</td>\n      <td>3.5</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022683E9EA5</td>\n      <td>When a problem is a change you have to let it do the best on you no matter what is happening it can change your mind. sometimes you need to wake up and look what is around you because problems are the best way to change what you want to change along time ago. A\\n\\nproblem is a change for you because it can make you see different and help you to understand how tings wok.\\n\\nFirst of all it can make you see different then the others. For example i remember that when i came to the United States i think that nothing was going to change me because i think that nothing was going to change me bec...</td>\n      <td>2.5</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00299B378633</td>\n      <td>Dear, Principal\\n\\nIf u change the school policy of having a grade b average that unfair. Because many students have a C average. So that means that they cant go out for sports or other activities they want to do bad. That's like taking everything they have. What if kids want to become good at something, but now they cant because of that school policy. If they have a C average they should still be able to go out for sports or activities. A C average isn't that bad, its higher then a D average. If the school police was if you have a D average of lower they shouldn't do sports or activities....</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>003885A45F42</td>\n      <td>The best time in life is when you become yourself. I agree that the greatest accomplishment, is when you be yourself in a world that constantly trying to make you something else. Because you make your own choices, you become more happy, and you respect others.\\n\\nFirst, you make your own choices by being yourself. Becoming yourself means that you should be able to make your own choices and not be shy or afraid of what you're doing. Because you're defining yourself by doing those things that you want. Some people follow others, therefore, they don't make their own choices. People are afraid...</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0049B1DF5CCC</td>\n      <td>Small act of kindness can impact in other people can change people to become better persons you can have an impact of kindess with a homeles that can change his life or with some who needed they are going to know you are a nice person if you are a nice person everywhere you go people is going to like your personality so you have to be a nice person with others like a old women triying to cross the road thats a impact of kindness when you do that you feel a greate person you can change people in the way they think by helping others treating nice other people give them some advice when you s...</td>\n      <td>2.5</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform basic exploratory data analysis\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "FULL_TEXT = \"full_text\"\n",
    "TEXT_ID = \"text_id\"\n",
    "TARGET_LABELS = {\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"}\n",
    "GRADES = [\"F\", \"D\", \"D+\", \"C\", \"C+\", \"B\", \"B+\", \"A\", \"A+\"]\n",
    "\n",
    "score_to_grade_dict = dict(zip(np.arange(1, 5.5, 0.5), GRADES))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/paritoshborkar/mambaforge/envs/ell-feedback/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Get tokenizer used by preexisting model\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "def tokenize_text(ds):\n",
    "    return tokenizer(ds[FULL_TEXT])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [1, 273, 428, 272, 598, 338, 1591, 292, 1101, 288, 425, 261, 10405, 306, 13521, 286, 264, 575, 263, 350, 322, 737, 267, 262, 1066, 264, 3038, 263, 333, 343, 1245, 260, 787, 364, 2141, 1530, 349, 401, 288, 343, 669, 306, 280, 436, 282, 794, 310, 1251, 260, 306, 296, 282, 1800, 288, 425, 260, 279, 9769, 465, 265, 563, 269, 646, 1041, 260, 274, 4302, 322, 424, 5063, 290, 3261, 263, 424, 264, 290, 6928, 263, 468, 288, 290, 36738, 260, 385, 274, 428, 274, 2847, 266, 5776, 3636, 424, 468, 267, 262, 4520, 263, 274, 436, 814, 298, 334, 278, 289, 274, 468, 263, 398, 266, 10997, 260, 1060, 274, 280, 436, 286, 264, 575, 260, 275, 262, 535, 2141, 274, 295, 1929, 784, 263, 992, 425, 263, 274, 13521, 389, 264, 2148, 314, 339, 264, 1929, 260, 370, 598, 1048, 413, 10796, 416, 563, 260, 306, 814, 413, 278, 416, 306, 1873, 289, 335, 306, 4302, 322, 260, 347, 598, 333, 462, 264, 4984, 397, 260, 272, 2884, 349, 333, 2374, 262, 2444, 263, 1563, 277, 343, 4178, 326, 1138, 306, 488, 1109, 264, 563, 260, 335, 3636, 286, 535, 2141, 3636, 13521, 389, 264, 2374, 3620, 1138, 274, 295, 350, 758, 487, 322, 263, 424, 413, 266, 3038, 263, 335, 3636, 350, 321, 290, 1041, 264, 424, 260, 335, 290, 425, 290, 1800, 263, 274, 794, 1251, 260, 278, 1360, 393, 299, 1951, 264, 282, 13245, 263, 402, 1633, 343, 16133, 277, 938, 374, 260, 613, 1657, 281, 1179, 402, 337, 274, 687, 260, 347, 2274, 6421, 391, 361, 264, 2900, 278, 267, 393, 384, 272, 598, 796, 278, 260, 272, 2884, 598, 264, 3703, 263, 306, 372, 5204, 262, 938, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "tokenize_text(ds[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize_text, batched=True, remove_columns=(FULL_TEXT, TEXT_ID))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[3.5,\n 2.5,\n 3.0,\n 4.5,\n 2.5,\n 3.5,\n 3.5,\n 2.5,\n 3.0,\n 3.0,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 4.0,\n 2.0,\n 3.0,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 2.0,\n 3.0,\n 4.0,\n 4.0,\n 2.0,\n 3.0,\n 4.5,\n 3.0,\n 2.5,\n 4.0,\n 3.0,\n 3.5,\n 3.5,\n 2.0,\n 4.0,\n 3.0,\n 4.0,\n 4.0,\n 1.0,\n 4.0,\n 2.5,\n 2.5,\n 3.5,\n 4.0,\n 2.5,\n 4.0,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 2.5,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 4.5,\n 2.0,\n 2.0,\n 2.5,\n 3.0,\n 2.0,\n 3.0,\n 3.5,\n 2.5,\n 3.5,\n 2.5,\n 3.5,\n 3.0,\n 3.0,\n 2.0,\n 3.0,\n 3.0,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 3.0,\n 2.0,\n 3.0,\n 4.0,\n 3.0,\n 2.0,\n 2.5,\n 2.5,\n 2.0,\n 3.5,\n 2.0,\n 2.0,\n 4.0,\n 2.5,\n 4.0,\n 3.5,\n 3.5,\n 4.0,\n 2.0,\n 2.0,\n 2.5,\n 3.5,\n 3.0,\n 5.0,\n 3.5,\n 2.5,\n 3.5,\n 3.0,\n 3.5,\n 2.5,\n 3.5,\n 3.0,\n 2.0,\n 2.5,\n 3.5,\n 3.0,\n 3.5,\n 2.5,\n 2.5,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 3.0,\n 2.0,\n 3.5,\n 4.0,\n 2.0,\n 3.0,\n 4.0,\n 3.5,\n 2.5,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.0,\n 3.0,\n 3.5,\n 4.5,\n 2.5,\n 3.5,\n 3.0,\n 4.0,\n 3.0,\n 2.0,\n 3.0,\n 3.5,\n 2.5,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 4.0,\n 4.0,\n 3.0,\n 3.5,\n 3.0,\n 4.0,\n 2.5,\n 2.0,\n 3.5,\n 2.5,\n 2.0,\n 2.0,\n 3.0,\n 2.5,\n 3.0,\n 4.0,\n 3.0,\n 2.0,\n 2.5,\n 1.5,\n 3.5,\n 3.0,\n 2.0,\n 3.5,\n 4.0,\n 4.0,\n 2.5,\n 2.0,\n 3.0,\n 3.0,\n 4.0,\n 2.5,\n 2.0,\n 3.0,\n 3.5,\n 3.5,\n 3.0,\n 3.5,\n 4.0,\n 2.5,\n 3.5,\n 4.0,\n 4.0,\n 3.0,\n 2.5,\n 3.0,\n 4.0,\n 3.0,\n 4.0,\n 2.0,\n 2.5,\n 2.0,\n 3.0,\n 2.5,\n 3.0,\n 4.5,\n 3.5,\n 3.5,\n 4.0,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 3.5,\n 3.0,\n 2.5,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 2.5,\n 2.5,\n 3.5,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 2.5,\n 4.0,\n 3.0,\n 3.0,\n 3.5,\n 4.0,\n 2.5,\n 4.0,\n 2.5,\n 3.5,\n 4.0,\n 4.0,\n 2.0,\n 3.0,\n 2.5,\n 2.5,\n 3.5,\n 3.5,\n 1.5,\n 3.0,\n 4.0,\n 2.0,\n 5.0,\n 2.5,\n 3.5,\n 4.5,\n 3.0,\n 2.5,\n 3.5,\n 3.5,\n 3.5,\n 2.5,\n 3.0,\n 2.5,\n 4.0,\n 3.0,\n 2.0,\n 3.5,\n 3.5,\n 3.5,\n 4.0,\n 2.5,\n 4.5,\n 2.5,\n 3.5,\n 3.0,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 3.5,\n 2.5,\n 2.0,\n 4.0,\n 2.5,\n 3.0,\n 3.5,\n 4.0,\n 3.5,\n 2.0,\n 3.0,\n 2.5,\n 3.5,\n 4.0,\n 4.5,\n 3.5,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 3.0,\n 2.5,\n 2.5,\n 3.0,\n 3.5,\n 3.0,\n 4.5,\n 3.5,\n 3.0,\n 3.0,\n 2.0,\n 3.0,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 3.5,\n 4.0,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.5,\n 4.0,\n 2.0,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 3.5,\n 4.5,\n 2.5,\n 4.5,\n 3.0,\n 4.0,\n 3.5,\n 3.5,\n 3.0,\n 2.0,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 4.0,\n 2.5,\n 3.0,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 2.5,\n 3.0,\n 2.0,\n 2.5,\n 2.5,\n 3.5,\n 2.5,\n 2.0,\n 3.5,\n 2.5,\n 3.0,\n 3.0,\n 1.5,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 2.5,\n 3.5,\n 4.5,\n 3.5,\n 3.0,\n 2.0,\n 2.5,\n 4.5,\n 2.0,\n 2.5,\n 3.0,\n 2.5,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 3.5,\n 3.0,\n 4.0,\n 2.5,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 3.0,\n 3.0,\n 3.5,\n 2.5,\n 4.5,\n 3.0,\n 2.0,\n 2.5,\n 4.0,\n 3.5,\n 3.5,\n 4.0,\n 3.5,\n 3.0,\n 4.5,\n 4.5,\n 3.5,\n 1.5,\n 2.0,\n 3.0,\n 2.0,\n 4.0,\n 3.5,\n 3.0,\n 2.5,\n 4.0,\n 3.5,\n 2.5,\n 4.5,\n 3.5,\n 2.5,\n 2.5,\n 3.5,\n 4.0,\n 3.5,\n 4.0,\n 2.5,\n 3.5,\n 2.5,\n 3.0,\n 3.5,\n 3.0,\n 2.0,\n 3.0,\n 3.5,\n 4.5,\n 3.0,\n 3.0,\n 3.5,\n 3.5,\n 4.0,\n 4.0,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 2.5,\n 2.5,\n 3.5,\n 3.0,\n 2.5,\n 4.0,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 2.5,\n 3.0,\n 5.0,\n 2.5,\n 3.0,\n 2.5,\n 4.5,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 3.5,\n 4.0,\n 3.0,\n 4.0,\n 3.5,\n 3.5,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.5,\n 4.5,\n 3.5,\n 2.5,\n 3.0,\n 4.0,\n 2.5,\n 3.5,\n 3.5,\n 2.0,\n 3.5,\n 2.5,\n 3.0,\n 3.0,\n 1.5,\n 3.0,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 2.5,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 4.0,\n 3.0,\n 3.5,\n 2.0,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.0,\n 3.5,\n 3.5,\n 2.5,\n 2.5,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 3.0,\n 4.0,\n 3.0,\n 2.0,\n 4.0,\n 2.5,\n 2.0,\n 3.5,\n 4.0,\n 3.0,\n 2.0,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 2.5,\n 3.0,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 2.5,\n 3.5,\n 2.5,\n 2.5,\n 2.0,\n 2.5,\n 3.5,\n 4.0,\n 3.0,\n 4.0,\n 4.0,\n 3.5,\n 3.5,\n 4.0,\n 2.5,\n 2.5,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 3.5,\n 3.5,\n 2.5,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 4.0,\n 3.5,\n 4.0,\n 3.0,\n 3.5,\n 3.5,\n 2.5,\n 3.0,\n 3.0,\n 3.5,\n 2.5,\n 3.0,\n 2.5,\n 3.0,\n 4.0,\n 3.5,\n 2.0,\n 3.0,\n 3.0,\n 4.5,\n 2.5,\n 2.0,\n 4.0,\n 4.0,\n 3.5,\n 4.0,\n 3.5,\n 2.0,\n 3.5,\n 4.0,\n 3.0,\n 3.0,\n 3.0,\n 2.0,\n 3.5,\n 2.5,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 3.0,\n 2.0,\n 2.5,\n 3.0,\n 3.0,\n 4.0,\n 4.0,\n 3.5,\n 2.5,\n 3.0,\n 2.5,\n 3.0,\n 4.0,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 3.5,\n 2.5,\n 2.5,\n 4.0,\n 3.0,\n 3.5,\n 4.0,\n 2.5,\n 3.5,\n 2.5,\n 5.0,\n 3.0,\n 2.5,\n 4.0,\n 2.5,\n 2.0,\n 3.0,\n 5.0,\n 2.5,\n 3.5,\n 2.5,\n 2.5,\n 1.5,\n 2.0,\n 2.5,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 4.0,\n 3.5,\n 3.0,\n 3.5,\n 4.0,\n 2.5,\n 3.0,\n 2.0,\n 4.5,\n 4.0,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 2.5,\n 3.5,\n 3.0,\n 4.0,\n 4.0,\n 3.0,\n 3.0,\n 3.0,\n 4.0,\n 2.0,\n 2.0,\n 3.5,\n 3.5,\n 4.0,\n 3.5,\n 4.5,\n 3.0,\n 2.0,\n 3.0,\n 4.0,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 3.0,\n 3.5,\n 2.5,\n 3.0,\n 2.5,\n 3.5,\n 3.0,\n 3.5,\n 3.5,\n 2.5,\n 4.0,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 2.0,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 3.5,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 4.0,\n 2.5,\n 2.5,\n 3.5,\n 4.0,\n 2.5,\n 3.0,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 2.5,\n 2.0,\n 2.5,\n 2.5,\n 3.5,\n 3.5,\n 4.5,\n 3.0,\n 3.5,\n 3.0,\n 3.5,\n 3.5,\n 4.5,\n 4.0,\n 5.0,\n 4.5,\n 4.0,\n 3.0,\n 4.5,\n 2.0,\n 2.5,\n 4.0,\n 3.0,\n 3.0,\n 2.0,\n 3.0,\n 2.0,\n 3.5,\n 3.0,\n 2.5,\n 2.0,\n 2.5,\n 3.5,\n 2.0,\n 3.0,\n 3.5,\n 3.0,\n 3.5,\n 4.0,\n 2.5,\n 2.5,\n 2.0,\n 2.5,\n 3.5,\n 4.0,\n 2.5,\n 2.5,\n 5.0,\n 3.5,\n 4.0,\n 2.0,\n 3.0,\n 3.5,\n 2.0,\n 4.0,\n 3.0,\n 2.0,\n 3.5,\n 4.0,\n 2.5,\n 3.0,\n 4.0,\n 3.0,\n 2.5,\n 4.5,\n 3.0,\n 3.0,\n 3.0,\n 3.0,\n 3.0,\n 3.5,\n 3.0,\n 2.5,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 3.0,\n 2.0,\n 3.0,\n 3.0,\n 3.5,\n 2.5,\n 2.0,\n 3.5,\n 4.0,\n 3.5,\n 2.5,\n 2.5,\n 2.5,\n 2.5,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 2.5,\n 3.0,\n 2.5,\n 3.5,\n 3.5,\n 4.0,\n 3.0,\n 4.0,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 3.0,\n 3.5,\n 4.0,\n 3.5,\n 4.0,\n 3.5,\n 4.0,\n 2.5,\n 3.5,\n 3.0,\n 2.5,\n 4.0,\n 3.5,\n 4.0,\n 4.0,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 3.0,\n 2.5,\n 3.0,\n 2.0,\n 3.0,\n 2.5,\n 3.0,\n 2.0,\n 2.5,\n 3.0,\n 3.5,\n 4.5,\n 3.5,\n 3.0,\n 3.5,\n 2.0,\n 4.0,\n 4.0,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 3.5,\n 3.5,\n 3.0,\n 2.0,\n 3.5,\n 3.0,\n 3.0,\n 3.5,\n 3.5,\n 4.5,\n 3.0,\n 4.0,\n 3.0,\n 5.0,\n 1.5,\n 3.0,\n 4.0,\n 4.5,\n 3.0,\n 3.5,\n 3.5,\n 3.5,\n 3.5,\n 3.5,\n 3.0,\n 4.5,\n 2.0,\n 3.0,\n 2.5,\n 4.5,\n 3.0,\n 3.0,\n 1.0,\n 3.0,\n 2.5,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 3.5,\n 3.0,\n 4.0,\n 3.5,\n 2.5,\n 2.5,\n 2.5,\n 2.5,\n 2.5,\n 3.5,\n 3.5,\n 3.0,\n 2.5,\n 3.0,\n 2.5,\n 3.5,\n 2.0,\n 3.5,\n 2.5,\n 3.0,\n 4.0,\n 3.0,\n 4.0,\n 4.5,\n 2.0,\n 4.0,\n 3.5,\n 3.5,\n 3.5,\n 3.5,\n 3.5,\n 2.5,\n 2.5,\n 2.5,\n 2.0,\n 3.0,\n 2.5,\n 3.5,\n 3.0,\n 3.5,\n 2.5,\n ...]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[\"cohesion\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Create HuggingFace datasets\n",
    "def get_datasets_dict(ds: Dataset):\n",
    "    return ds.train_test_split(0.25, seed=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "datasets_dict = get_datasets_dict(tokenized_ds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Map scores to equivalent grades\n",
    "def map_score_to_grade(list_scores):\n",
    "    return list(map(lambda score: score_to_grade_dict[score], list_scores))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 ms, sys: 118 µs, total: 2.7 ms\n",
      "Wall time: 2.77 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "['B',\n 'C',\n 'C+',\n 'A',\n 'C',\n 'B',\n 'B',\n 'C',\n 'C+',\n 'C+',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'B+',\n 'D+',\n 'C+',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'D+',\n 'C+',\n 'B+',\n 'B+',\n 'D+',\n 'C+',\n 'A',\n 'C+',\n 'C',\n 'B+',\n 'C+',\n 'B',\n 'B',\n 'D+',\n 'B+',\n 'C+',\n 'B+',\n 'B+',\n 'F',\n 'B+',\n 'C',\n 'C',\n 'B',\n 'B+',\n 'C',\n 'B+',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'C',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'A',\n 'D+',\n 'D+',\n 'C',\n 'C+',\n 'D+',\n 'C+',\n 'B',\n 'C',\n 'B',\n 'C',\n 'B',\n 'C+',\n 'C+',\n 'D+',\n 'C+',\n 'C+',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'C+',\n 'D+',\n 'C+',\n 'B+',\n 'C+',\n 'D+',\n 'C',\n 'C',\n 'D+',\n 'B',\n 'D+',\n 'D+',\n 'B+',\n 'C',\n 'B+',\n 'B',\n 'B',\n 'B+',\n 'D+',\n 'D+',\n 'C',\n 'B',\n 'C+',\n 'A+',\n 'B',\n 'C',\n 'B',\n 'C+',\n 'B',\n 'C',\n 'B',\n 'C+',\n 'D+',\n 'C',\n 'B',\n 'C+',\n 'B',\n 'C',\n 'C',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'C+',\n 'D+',\n 'B',\n 'B+',\n 'D+',\n 'C+',\n 'B+',\n 'B',\n 'C',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'C+',\n 'C+',\n 'B',\n 'A',\n 'C',\n 'B',\n 'C+',\n 'B+',\n 'C+',\n 'D+',\n 'C+',\n 'B',\n 'C',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'B+',\n 'B+',\n 'C+',\n 'B',\n 'C+',\n 'B+',\n 'C',\n 'D+',\n 'B',\n 'C',\n 'D+',\n 'D+',\n 'C+',\n 'C',\n 'C+',\n 'B+',\n 'C+',\n 'D+',\n 'C',\n 'D',\n 'B',\n 'C+',\n 'D+',\n 'B',\n 'B+',\n 'B+',\n 'C',\n 'D+',\n 'C+',\n 'C+',\n 'B+',\n 'C',\n 'D+',\n 'C+',\n 'B',\n 'B',\n 'C+',\n 'B',\n 'B+',\n 'C',\n 'B',\n 'B+',\n 'B+',\n 'C+',\n 'C',\n 'C+',\n 'B+',\n 'C+',\n 'B+',\n 'D+',\n 'C',\n 'D+',\n 'C+',\n 'C',\n 'C+',\n 'A',\n 'B',\n 'B',\n 'B+',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'B',\n 'C+',\n 'C',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'C',\n 'C',\n 'B',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'C',\n 'B+',\n 'C+',\n 'C+',\n 'B',\n 'B+',\n 'C',\n 'B+',\n 'C',\n 'B',\n 'B+',\n 'B+',\n 'D+',\n 'C+',\n 'C',\n 'C',\n 'B',\n 'B',\n 'D',\n 'C+',\n 'B+',\n 'D+',\n 'A+',\n 'C',\n 'B',\n 'A',\n 'C+',\n 'C',\n 'B',\n 'B',\n 'B',\n 'C',\n 'C+',\n 'C',\n 'B+',\n 'C+',\n 'D+',\n 'B',\n 'B',\n 'B',\n 'B+',\n 'C',\n 'A',\n 'C',\n 'B',\n 'C+',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'B',\n 'C',\n 'D+',\n 'B+',\n 'C',\n 'C+',\n 'B',\n 'B+',\n 'B',\n 'D+',\n 'C+',\n 'C',\n 'B',\n 'B+',\n 'A',\n 'B',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'C+',\n 'C',\n 'C',\n 'C+',\n 'B',\n 'C+',\n 'A',\n 'B',\n 'C+',\n 'C+',\n 'D+',\n 'C+',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'B',\n 'B+',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'B',\n 'B+',\n 'D+',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'B',\n 'A',\n 'C',\n 'A',\n 'C+',\n 'B+',\n 'B',\n 'B',\n 'C+',\n 'D+',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'B+',\n 'C',\n 'C+',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'C',\n 'C+',\n 'D+',\n 'C',\n 'C',\n 'B',\n 'C',\n 'D+',\n 'B',\n 'C',\n 'C+',\n 'C+',\n 'D',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'C',\n 'B',\n 'A',\n 'B',\n 'C+',\n 'D+',\n 'C',\n 'A',\n 'D+',\n 'C',\n 'C+',\n 'C',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'B',\n 'C+',\n 'B+',\n 'C',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'C+',\n 'C+',\n 'B',\n 'C',\n 'A',\n 'C+',\n 'D+',\n 'C',\n 'B+',\n 'B',\n 'B',\n 'B+',\n 'B',\n 'C+',\n 'A',\n 'A',\n 'B',\n 'D',\n 'D+',\n 'C+',\n 'D+',\n 'B+',\n 'B',\n 'C+',\n 'C',\n 'B+',\n 'B',\n 'C',\n 'A',\n 'B',\n 'C',\n 'C',\n 'B',\n 'B+',\n 'B',\n 'B+',\n 'C',\n 'B',\n 'C',\n 'C+',\n 'B',\n 'C+',\n 'D+',\n 'C+',\n 'B',\n 'A',\n 'C+',\n 'C+',\n 'B',\n 'B',\n 'B+',\n 'B+',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'C',\n 'C',\n 'B',\n 'C+',\n 'C',\n 'B+',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'C',\n 'C+',\n 'A+',\n 'C',\n 'C+',\n 'C',\n 'A',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'B',\n 'B+',\n 'C+',\n 'B+',\n 'B',\n 'B',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'B',\n 'A',\n 'B',\n 'C',\n 'C+',\n 'B+',\n 'C',\n 'B',\n 'B',\n 'D+',\n 'B',\n 'C',\n 'C+',\n 'C+',\n 'D',\n 'C+',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'C',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'B+',\n 'C+',\n 'B',\n 'D+',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'C+',\n 'B',\n 'B',\n 'C',\n 'C',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'C+',\n 'B+',\n 'C+',\n 'D+',\n 'B+',\n 'C',\n 'D+',\n 'B',\n 'B+',\n 'C+',\n 'D+',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'C',\n 'C+',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'C',\n 'B',\n 'C',\n 'C',\n 'D+',\n 'C',\n 'B',\n 'B+',\n 'C+',\n 'B+',\n 'B+',\n 'B',\n 'B',\n 'B+',\n 'C',\n 'C',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'B',\n 'B',\n 'C',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'B+',\n 'B',\n 'B+',\n 'C+',\n 'B',\n 'B',\n 'C',\n 'C+',\n 'C+',\n 'B',\n 'C',\n 'C+',\n 'C',\n 'C+',\n 'B+',\n 'B',\n 'D+',\n 'C+',\n 'C+',\n 'A',\n 'C',\n 'D+',\n 'B+',\n 'B+',\n 'B',\n 'B+',\n 'B',\n 'D+',\n 'B',\n 'B+',\n 'C+',\n 'C+',\n 'C+',\n 'D+',\n 'B',\n 'C',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'C+',\n 'D+',\n 'C',\n 'C+',\n 'C+',\n 'B+',\n 'B+',\n 'B',\n 'C',\n 'C+',\n 'C',\n 'C+',\n 'B+',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'B',\n 'C',\n 'C',\n 'B+',\n 'C+',\n 'B',\n 'B+',\n 'C',\n 'B',\n 'C',\n 'A+',\n 'C+',\n 'C',\n 'B+',\n 'C',\n 'D+',\n 'C+',\n 'A+',\n 'C',\n 'B',\n 'C',\n 'C',\n 'D',\n 'D+',\n 'C',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'B+',\n 'B',\n 'C+',\n 'B',\n 'B+',\n 'C',\n 'C+',\n 'D+',\n 'A',\n 'B+',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'C',\n 'B',\n 'C+',\n 'B+',\n 'B+',\n 'C+',\n 'C+',\n 'C+',\n 'B+',\n 'D+',\n 'D+',\n 'B',\n 'B',\n 'B+',\n 'B',\n 'A',\n 'C+',\n 'D+',\n 'C+',\n 'B+',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'C+',\n 'B',\n 'C',\n 'C+',\n 'C',\n 'B',\n 'C+',\n 'B',\n 'B',\n 'C',\n 'B+',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'D+',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'B',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'B+',\n 'C',\n 'C',\n 'B',\n 'B+',\n 'C',\n 'C+',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'C',\n 'D+',\n 'C',\n 'C',\n 'B',\n 'B',\n 'A',\n 'C+',\n 'B',\n 'C+',\n 'B',\n 'B',\n 'A',\n 'B+',\n 'A+',\n 'A',\n 'B+',\n 'C+',\n 'A',\n 'D+',\n 'C',\n 'B+',\n 'C+',\n 'C+',\n 'D+',\n 'C+',\n 'D+',\n 'B',\n 'C+',\n 'C',\n 'D+',\n 'C',\n 'B',\n 'D+',\n 'C+',\n 'B',\n 'C+',\n 'B',\n 'B+',\n 'C',\n 'C',\n 'D+',\n 'C',\n 'B',\n 'B+',\n 'C',\n 'C',\n 'A+',\n 'B',\n 'B+',\n 'D+',\n 'C+',\n 'B',\n 'D+',\n 'B+',\n 'C+',\n 'D+',\n 'B',\n 'B+',\n 'C',\n 'C+',\n 'B+',\n 'C+',\n 'C',\n 'A',\n 'C+',\n 'C+',\n 'C+',\n 'C+',\n 'C+',\n 'B',\n 'C+',\n 'C',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'C+',\n 'D+',\n 'C+',\n 'C+',\n 'B',\n 'C',\n 'D+',\n 'B',\n 'B+',\n 'B',\n 'C',\n 'C',\n 'C',\n 'C',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'C',\n 'C+',\n 'C',\n 'B',\n 'B',\n 'B+',\n 'C+',\n 'B+',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'C+',\n 'B',\n 'B+',\n 'B',\n 'B+',\n 'B',\n 'B+',\n 'C',\n 'B',\n 'C+',\n 'C',\n 'B+',\n 'B',\n 'B+',\n 'B+',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'C+',\n 'C',\n 'C+',\n 'D+',\n 'C+',\n 'C',\n 'C+',\n 'D+',\n 'C',\n 'C+',\n 'B',\n 'A',\n 'B',\n 'C+',\n 'B',\n 'D+',\n 'B+',\n 'B+',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'B',\n 'B',\n 'C+',\n 'D+',\n 'B',\n 'C+',\n 'C+',\n 'B',\n 'B',\n 'A',\n 'C+',\n 'B+',\n 'C+',\n 'A+',\n 'D',\n 'C+',\n 'B+',\n 'A',\n 'C+',\n 'B',\n 'B',\n 'B',\n 'B',\n 'B',\n 'C+',\n 'A',\n 'D+',\n 'C+',\n 'C',\n 'A',\n 'C+',\n 'C+',\n 'F',\n 'C+',\n 'C',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'B',\n 'C+',\n 'B+',\n 'B',\n 'C',\n 'C',\n 'C',\n 'C',\n 'C',\n 'B',\n 'B',\n 'C+',\n 'C',\n 'C+',\n 'C',\n 'B',\n 'D+',\n 'B',\n 'C',\n 'C+',\n 'B+',\n 'C+',\n 'B+',\n 'A',\n 'D+',\n 'B+',\n 'B',\n 'B',\n 'B',\n 'B',\n 'B',\n 'C',\n 'C',\n 'C',\n 'D+',\n 'C+',\n 'C',\n 'B',\n 'C+',\n 'B',\n 'C',\n ...]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "map_score_to_grade(ds[\"cohesion\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Create metric evaluation function\n",
    "def metric_func(*eval_predictions):\n",
    "    pred_values = map_score_to_grade(tokenized_ds[\"cohesion\"])\n",
    "    true_values = map_score_to_grade(tokenized_ds[\"syntax\"])\n",
    "    return confusion_matrix(pred_values, true_values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 ms, sys: 4.2 ms, total: 23.5 ms\n",
      "Wall time: 23.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 33,   7,  21,  57,   0,   7,   0,   0,   0],\n       [ 10,   7,   1,   8,   0,   0,   0,   0,   0],\n       [ 10,   0, 370, 119, 107, 373,   0,   9,   0],\n       [ 46,   3, 190, 156,   8, 131,   0,   0,   0],\n       [  0,   0,  38,   3, 332, 233,   5, 179,   0],\n       [  1,   0, 246,  45, 282, 440,   0,  82,   0],\n       [  0,   0,   0,   0,   1,   0,  13,  10,   3],\n       [  0,   0,   1,   0, 109,  66,   6, 130,   3],\n       [  0,   0,   0,   0,   0,   0,   5,   0,   5]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "metric_func()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "learning_rate, batch_size = 1e-5, 128\n",
    "num_epochs = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module transformers.trainer:\n",
      "\n",
      "class Trainer(builtins.object)\n",
      " |  Trainer(model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None, args: transformers.training_args.TrainingArguments = None, data_collator: Optional[transformers.data.data_collator.DataCollator] = None, train_dataset: Optional[torch.utils.data.dataset.Dataset] = None, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, tokenizer: Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None, model_init: Optional[Callable[[], transformers.modeling_utils.PreTrainedModel]] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None)\n",
      " |  \n",
      " |  Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.\n",
      " |  \n",
      " |  Args:\n",
      " |      model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n",
      " |          The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n",
      " |  \n",
      " |          <Tip>\n",
      " |  \n",
      " |          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n",
      " |          your own models defined as `torch.nn.Module` as long as they work the same way as the 🤗 Transformers\n",
      " |          models.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      args ([`TrainingArguments`], *optional*):\n",
      " |          The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n",
      " |          `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n",
      " |      data_collator (`DataCollator`, *optional*):\n",
      " |          The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n",
      " |          default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n",
      " |          [`DataCollatorWithPadding`] otherwise.\n",
      " |      train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\n",
      " |          The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |          `model.forward()` method are automatically removed.\n",
      " |  \n",
      " |          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n",
      " |          distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n",
      " |          `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n",
      " |          manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n",
      " |          sets the seed of the RNGs used.\n",
      " |      eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\n",
      " |           The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |           `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n",
      " |           dataset prepending the dictionary key to the metric name.\n",
      " |      tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n",
      " |          The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n",
      " |          maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n",
      " |          interrupted training or reuse the fine-tuned model.\n",
      " |      model_init (`Callable[[], PreTrainedModel]`, *optional*):\n",
      " |          A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n",
      " |          from a new instance of the model as given by this function.\n",
      " |  \n",
      " |          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n",
      " |          be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n",
      " |          inner layers, dropout probabilities etc).\n",
      " |      compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n",
      " |          The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n",
      " |          a dictionary string to metric values.\n",
      " |      callbacks (List of [`TrainerCallback`], *optional*):\n",
      " |          A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n",
      " |          detailed in [here](callback).\n",
      " |  \n",
      " |          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n",
      " |      optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*): A tuple\n",
      " |          containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your model\n",
      " |          and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n",
      " |      preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n",
      " |          A function that preprocess the logits right before caching them at each evaluation step. Must take two\n",
      " |          tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n",
      " |          by this function will be reflected in the predictions received by `compute_metrics`.\n",
      " |  \n",
      " |          Note that the labels (second parameter) will be `None` if the dataset does not have them.\n",
      " |  \n",
      " |  Important attributes:\n",
      " |  \n",
      " |      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n",
      " |        subclass.\n",
      " |      - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n",
      " |        original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n",
      " |        the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n",
      " |        model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n",
      " |      - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n",
      " |        data parallelism, this means some of the model layers are split on different GPUs).\n",
      " |      - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n",
      " |        to `False` if model parallel or deepspeed is used, or if the default\n",
      " |        `TrainingArguments.place_model_on_device` is overridden to return `False` .\n",
      " |      - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n",
      " |        in `train`)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None, args: transformers.training_args.TrainingArguments = None, data_collator: Optional[transformers.data.data_collator.DataCollator] = None, train_dataset: Optional[torch.utils.data.dataset.Dataset] = None, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, tokenizer: Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None, model_init: Optional[Callable[[], transformers.modeling_utils.PreTrainedModel]] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add_callback(self, callback)\n",
      " |      Add a callback to the current list of [`~transformer.TrainerCallback`].\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformer.TrainerCallback`]):\n",
      " |             A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n",
      " |             first case, will instantiate a member of that class.\n",
      " |  \n",
      " |  autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True)\n",
      " |      A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n",
      " |      arguments, depending on the situation.\n",
      " |  \n",
      " |  call_model_init(self, trial=None)\n",
      " |  \n",
      " |  compute_loss(self, model, inputs, return_outputs=False)\n",
      " |      How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
      " |      \n",
      " |      Subclass and override for custom behavior.\n",
      " |  \n",
      " |  compute_loss_context_manager(self)\n",
      " |      A helper wrapper to group together context managers.\n",
      " |  \n",
      " |  create_model_card(self, language: Optional[str] = None, license: Optional[str] = None, tags: Union[str, List[str], NoneType] = None, model_name: Optional[str] = None, finetuned_from: Optional[str] = None, tasks: Union[str, List[str], NoneType] = None, dataset_tags: Union[str, List[str], NoneType] = None, dataset: Union[str, List[str], NoneType] = None, dataset_args: Union[str, List[str], NoneType] = None)\n",
      " |      Creates a draft of a model card using the information available to the `Trainer`.\n",
      " |      \n",
      " |      Args:\n",
      " |          language (`str`, *optional*):\n",
      " |              The language of the model (if applicable)\n",
      " |          license (`str`, *optional*):\n",
      " |              The license of the model. Will default to the license of the pretrained model used, if the original\n",
      " |              model given to the `Trainer` comes from a repo on the Hub.\n",
      " |          tags (`str` or `List[str]`, *optional*):\n",
      " |              Some tags to be included in the metadata of the model card.\n",
      " |          model_name (`str`, *optional*):\n",
      " |              The name of the model.\n",
      " |          finetuned_from (`str`, *optional*):\n",
      " |              The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n",
      " |              of the original model given to the `Trainer` (if it comes from the Hub).\n",
      " |          tasks (`str` or `List[str]`, *optional*):\n",
      " |              One or several task identifiers, to be included in the metadata of the model card.\n",
      " |          dataset_tags (`str` or `List[str]`, *optional*):\n",
      " |              One or several dataset tags, to be included in the metadata of the model card.\n",
      " |          dataset (`str` or `List[str]`, *optional*):\n",
      " |              One or several dataset identifiers, to be included in the metadata of the model card.\n",
      " |          dataset_args (`str` or `List[str]`, *optional*):\n",
      " |             One or several dataset arguments, to be included in the metadata of the model card.\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Setup the optimizer.\n",
      " |      \n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
      " |  \n",
      " |  create_optimizer_and_scheduler(self, num_training_steps: int)\n",
      " |      Setup the optimizer and the learning rate scheduler.\n",
      " |      \n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n",
      " |      `create_scheduler`) in a subclass.\n",
      " |  \n",
      " |  create_scheduler(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)\n",
      " |      Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n",
      " |      passed as an argument.\n",
      " |      \n",
      " |      Args:\n",
      " |          num_training_steps (int): The number of training steps to do.\n",
      " |  \n",
      " |  evaluate(self, eval_dataset: Optional[torch.utils.data.dataset.Dataset] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> Dict[str, float]\n",
      " |      Run evaluation and returns metrics.\n",
      " |      \n",
      " |      The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
      " |      (pass it to the init `compute_metrics` argument).\n",
      " |      \n",
      " |      You can also subclass and override this method to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          eval_dataset (`Dataset`, *optional*):\n",
      " |              Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n",
      " |              not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n",
      " |              method.\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"eval_bleu\" if the prefix is \"eval\" (default)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
      " |          dictionary also contains the epoch number which comes from the training state.\n",
      " |  \n",
      " |  evaluation_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |      \n",
      " |      Works both with or without labels.\n",
      " |  \n",
      " |  floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]])\n",
      " |      For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n",
      " |      operations for every backward + forward pass. If using another model, either implement such a method in the\n",
      " |      model or subclass and override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of floating-point operations.\n",
      " |  \n",
      " |  get_eval_dataloader(self, eval_dataset: Optional[torch.utils.data.dataset.Dataset] = None) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          eval_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |              If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n",
      " |              by the `model.forward()` method are automatically removed. It must implement `__len__`.\n",
      " |  \n",
      " |  get_test_dataloader(self, test_dataset: torch.utils.data.dataset.Dataset) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the test [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |              The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. It must implement `__len__`.\n",
      " |  \n",
      " |  get_train_dataloader(self) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the training [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
      " |      training if necessary) otherwise.\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |  \n",
      " |  hyperparameter_search(self, hp_space: Optional[Callable[[ForwardRef('optuna.Trial')], Dict[str, float]]] = None, compute_objective: Optional[Callable[[Dict[str, float]], float]] = None, n_trials: int = 20, direction: str = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Optional[Callable[[ForwardRef('optuna.Trial')], str]] = None, **kwargs) -> transformers.trainer_utils.BestRun\n",
      " |      Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n",
      " |      by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n",
      " |      the sum of all metrics otherwise.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\n",
      " |      reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\n",
      " |      subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\n",
      " |      optimizer/scheduler.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n",
      " |              A function that defines the hyperparameter search space. Will default to\n",
      " |              [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n",
      " |              [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n",
      " |          compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n",
      " |              A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n",
      " |              method. Will default to [`~trainer_utils.default_compute_objective`].\n",
      " |          n_trials (`int`, *optional*, defaults to 100):\n",
      " |              The number of trial runs to test.\n",
      " |          direction (`str`, *optional*, defaults to `\"minimize\"`):\n",
      " |              Whether to optimize greater or lower objects. Can be `\"minimize\"` or `\"maximize\"`, you should pick\n",
      " |              `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or several metrics.\n",
      " |          backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n",
      " |              The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n",
      " |              on which one is installed. If all are installed, will default to optuna.\n",
      " |          hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n",
      " |              A function that defines the trial/run name. Will default to None.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\n",
      " |              information see:\n",
      " |      \n",
      " |              - the documentation of\n",
      " |                [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n",
      " |              - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\n",
      " |              - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`trainer_utils.BestRun`]: All the information about the best run. Experiment summary can be found in\n",
      " |          `run_summary` attribute for Ray backend.\n",
      " |  \n",
      " |  init_git_repo(self, at_init: bool = False)\n",
      " |      Initializes a git repo in `self.args.hub_model_id`.\n",
      " |      \n",
      " |      Args:\n",
      " |          at_init (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is\n",
      " |              `True` and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped\n",
      " |              out.\n",
      " |  \n",
      " |  ipex_optimize_model(self, model, training=False, dtype=torch.float32)\n",
      " |  \n",
      " |  is_local_process_zero(self) -> bool\n",
      " |      Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n",
      " |      machines) main process.\n",
      " |  \n",
      " |  is_world_process_zero(self) -> bool\n",
      " |      Whether or not this process is the global main process (when training in a distributed fashion on several\n",
      " |      machines, this is only going to be `True` for one process).\n",
      " |  \n",
      " |  log(self, logs: Dict[str, float]) -> None\n",
      " |      Log `logs` on the various objects watching training.\n",
      " |      \n",
      " |      Subclass and override this method to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs (`Dict[str, float]`):\n",
      " |              The values to log.\n",
      " |  \n",
      " |  log_metrics(self, split, metrics)\n",
      " |      Log metrics in a specially formatted way\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |      \n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predictmetrics: metrics dict\n",
      " |      \n",
      " |      Notes on memory reports:\n",
      " |      \n",
      " |      In order to get memory usage report you need to install `psutil`. You can do that with `pip install psutil`.\n",
      " |      \n",
      " |      Now when this method is run, you will see a report that will include: :\n",
      " |      \n",
      " |      ```\n",
      " |      init_mem_cpu_alloc_delta   =     1301MB\n",
      " |      init_mem_cpu_peaked_delta  =      154MB\n",
      " |      init_mem_gpu_alloc_delta   =      230MB\n",
      " |      init_mem_gpu_peaked_delta  =        0MB\n",
      " |      train_mem_cpu_alloc_delta  =     1345MB\n",
      " |      train_mem_cpu_peaked_delta =        0MB\n",
      " |      train_mem_gpu_alloc_delta  =      693MB\n",
      " |      train_mem_gpu_peaked_delta =        7MB\n",
      " |      ```\n",
      " |      \n",
      " |      **Understanding the reports:**\n",
      " |      \n",
      " |      - the first segment, e.g., `train__`, tells you which stage the metrics are for. Reports starting with `init_`\n",
      " |          will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the\n",
      " |          `__init__` will be reported along with the `eval_` metrics.\n",
      " |      - the third segment, is either `cpu` or `gpu`, tells you whether it's the general RAM or the gpu0 memory\n",
      " |          metric.\n",
      " |      - `*_alloc_delta` - is the difference in the used/allocated memory counter between the end and the start of the\n",
      " |          stage - it can be negative if a function released more memory than it allocated.\n",
      " |      - `*_peaked_delta` - is any extra memory that was consumed and then freed - relative to the current allocated\n",
      " |          memory counter - it is never negative. When you look at the metrics of any stage you add up `alloc_delta` +\n",
      " |          `peaked_delta` and you know how much memory was needed to complete that stage.\n",
      " |      \n",
      " |      The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the\n",
      " |      main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may\n",
      " |      use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more\n",
      " |      memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the\n",
      " |      future these reports will evolve to measure those too.\n",
      " |      \n",
      " |      The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the\n",
      " |      memory shared with other processes. It is important to note that it does not include swapped out memory, so the\n",
      " |      reports could be imprecise.\n",
      " |      \n",
      " |      The CPU peak memory is measured using a sampling thread. Due to python's GIL it may miss some of the peak memory if\n",
      " |      that thread didn't get a chance to run when the highest memory was used. Therefore this report can be less than\n",
      " |      reality. Using `tracemalloc` would have reported the exact peak memory, but it doesn't report memory allocations\n",
      " |      outside of python. So if some C++ CUDA extension allocated its own memory it won't be reported. And therefore it\n",
      " |      was dropped in favor of the memory sampling approach, which reads the current process memory usage.\n",
      " |      \n",
      " |      The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()` and\n",
      " |      `torch.cuda.max_memory_allocated()`. This metric reports only \"deltas\" for pytorch-specific allocations, as\n",
      " |      `torch.cuda` memory management system doesn't track any memory allocated outside of pytorch. For example, the very\n",
      " |      first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.\n",
      " |      \n",
      " |      Note that this tracker doesn't account for memory allocations outside of [`Trainer`]'s `__init__`, `train`,\n",
      " |      `evaluate` and `predict` calls.\n",
      " |      \n",
      " |      Because `evaluation` calls may happen during `train`, we can't handle nested invocations because\n",
      " |      `torch.cuda.max_memory_allocated` is a single counter, so if it gets reset by a nested eval call, `train`'s tracker\n",
      " |      will report incorrect info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266) gets resolved\n",
      " |      it will be possible to change this class to be re-entrant. Until then we will only track the outer level of\n",
      " |      `train`, `evaluate` and `predict` methods. Which means that if `eval` is called during `train`, it's the latter\n",
      " |      that will account for its memory usage and that of the former.\n",
      " |      \n",
      " |      This also means that if any other tool that is used along the [`Trainer`] calls\n",
      " |      `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be invalid. And the [`Trainer`] will disrupt\n",
      " |      the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats` themselves.\n",
      " |      \n",
      " |      For best performance you may want to consider turning the memory profiling off for production runs.\n",
      " |  \n",
      " |  metrics_format(self, metrics: Dict[str, float]) -> Dict[str, float]\n",
      " |      Reformat Trainer metrics values to a human-readable format\n",
      " |      \n",
      " |      Args:\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |      \n",
      " |      Returns:\n",
      " |          metrics (`Dict[str, float]`): The reformatted metrics\n",
      " |  \n",
      " |  num_examples(self, dataloader: torch.utils.data.dataloader.DataLoader) -> int\n",
      " |      Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\n",
      " |      dataloader.dataset does not exist or has no length, estimates as best it can\n",
      " |  \n",
      " |  pop_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformer.TrainerCallback`] and returns it.\n",
      " |      \n",
      " |      If the callback is not found, returns `None` (and no error is raised).\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformer.TrainerCallback`]):\n",
      " |             A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n",
      " |             first case, will pop the first member of that class found in the list of callbacks.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`~transformer.TrainerCallback`]: The callback removed, if found.\n",
      " |  \n",
      " |  predict(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'test') -> transformers.trainer_utils.PredictionOutput\n",
      " |      Run prediction and returns predictions and potential metrics.\n",
      " |      \n",
      " |      Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n",
      " |      will also return metrics, like in `evaluate()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_dataset (`Dataset`):\n",
      " |              Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. Has to implement the method `__len__`\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"test_bleu\" if the prefix is \"test\" (default)\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If your predictions or labels have different sequence length (for instance because you're doing dynamic padding\n",
      " |      in a token classification task) the predictions will be padded (on the right) to allow for concatenation into\n",
      " |      one array. The padding index is -100.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Returns: *NamedTuple* A namedtuple with the following keys:\n",
      " |      \n",
      " |          - predictions (`np.ndarray`): The predictions on `test_dataset`.\n",
      " |          - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n",
      " |          - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n",
      " |            labels).\n",
      " |  \n",
      " |  prediction_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |      \n",
      " |      Works both with or without labels.\n",
      " |  \n",
      " |  prediction_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n",
      " |      Perform an evaluation step on `model` using `inputs`.\n",
      " |      \n",
      " |      Subclass and override to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to evaluate.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |          prediction_loss_only (`bool`):\n",
      " |              Whether or not to return the loss only.\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |      \n",
      " |      Return:\n",
      " |          Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
      " |          logits and labels (each being optional).\n",
      " |  \n",
      " |  push_to_hub(self, commit_message: Optional[str] = 'End of training', blocking: bool = True, **kwargs) -> str\n",
      " |      Upload *self.model* and *self.tokenizer* to the 🤗 model hub on the repo *self.args.hub_model_id*.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n",
      " |              Message to commit while pushing.\n",
      " |          blocking (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the function should return only when the `git push` has finished.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of\n",
      " |          the commit and an object to track the progress of the commit if `blocking=True`\n",
      " |  \n",
      " |  remove_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformer.TrainerCallback`].\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformer.TrainerCallback`]):\n",
      " |             A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n",
      " |             first case, will remove the first member of that class found in the list of callbacks.\n",
      " |  \n",
      " |  save_metrics(self, split, metrics, combined=True)\n",
      " |      Save metrics into a json file for that split, e.g. `train_results.json`.\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |      \n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`, `all`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |          combined (`bool`, *optional*, defaults to `True`):\n",
      " |              Creates combined metrics by updating `all_results.json` with metrics of this call\n",
      " |      \n",
      " |      To understand the metrics please read the docstring of [`~Trainer.log_metrics`]. The only difference is that raw\n",
      " |      unformatted numbers are saved in the current method.\n",
      " |  \n",
      " |  save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False)\n",
      " |      Will save the model, so you can reload it using `from_pretrained()`.\n",
      " |      \n",
      " |      Will only save from the main process.\n",
      " |  \n",
      " |  save_state(self)\n",
      " |      Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |  \n",
      " |  store_flos(self)\n",
      " |  \n",
      " |  torch_jit_model_eval(self, model, dataloader, training=False)\n",
      " |  \n",
      " |  train(self, resume_from_checkpoint: Union[bool, str, NoneType] = None, trial: Union[ForwardRef('optuna.Trial'), Dict[str, Any]] = None, ignore_keys_for_eval: Optional[List[str]] = None, **kwargs)\n",
      " |      Main training entry point.\n",
      " |      \n",
      " |      Args:\n",
      " |          resume_from_checkpoint (`str` or `bool`, *optional*):\n",
      " |              If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n",
      " |              `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n",
      " |              of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
      " |          trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n",
      " |              The trial run or the hyperparameter dictionary for hyperparameter search.\n",
      " |          ignore_keys_for_eval (`List[str]`, *optional*)\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions for evaluation during the training.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments used to hide deprecated arguments\n",
      " |  \n",
      " |  training_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor\n",
      " |      Perform a training step on a batch of inputs.\n",
      " |      \n",
      " |      Subclass and override to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to train.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |      \n",
      " |      Return:\n",
      " |          `torch.Tensor`: The tensor with training loss on this batch.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_optimizer_cls_and_kwargs(args: transformers.training_args.TrainingArguments) -> Tuple[Any, Any]\n",
      " |      Returns the optimizer class and optimizer parameters based on the training arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (`transformers.training_args.TrainingArguments`):\n",
      " |              The training arguments for the training session.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Trainer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# Create a simple model for making predictions\n",
    "def get_trainer():\n",
    "    training_args = TrainingArguments('outputs', learning_rate=learning_rate, warmup_ratio=0.1, lr_scheduler_type='cosine',\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size*2,\n",
    "    num_train_epochs=num_epochs, weight_decay=0.01, report_to='none')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(TARGET_LABELS))\n",
    "    return Trainer(model, training_args, train_dataset=datasets_dict['train'], eval_dataset=datasets_dict['test'],\n",
    "                   tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 286M/286M [00:05<00:00, 48.5MB/s] \n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = get_trainer()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paritoshborkar/mambaforge/envs/ell-feedback/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data into dataloaders with transformations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train and validate model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train on complete training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions on test data and create submission file"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
